---
title: "Overview"
description: "OpenPipe is a streamlined platform designed to help product-focused teams train specialized LLM models as replacements for slow and expensive prompts."
---

## Who We Are

We're a team of full-stack engineers and machine learning researchers working to streamline the process of integrating fine-tuned models into any application. Our goal is to make the fine-tuning process accessible to everyone.

## What We Provide

Here are a few of the features we offer:

- [**Data Capture**](/getting-started/openpipe-sdk): OpenPipe automatically captures every request and response sent through our drop-in replacement sdk and stores it for your future use.

- [**Import Data**](/features/importing-data): OpenPipe also allows you to import data for fine-tuning from OpenAI-compatible JSONL files.

- [**Request Logs**](/features/request-logs): We help you log your past requests and tag them for easy filtering.

- [**Fine-Tuning**](/features/fine-tuning): With all your LLM requests and responses in one place, it's easy to select the data you want to fine-tune on and kick off a job.

- [**Evaluations**](/features/evaluations): Compare your models against one another and OpenAI base models. Set up custom instructions and get quick insights into your models' performance.

- [**Model Hosting**](/features/running-inference): After we've trained your model, OpenPipe will automatically begin hosting it. Accessing your model will require an API key from your project.

- [**Pruning Rules**](/features/pruning-rules): By removing large chunks of unchanging text and fine-tuning a model on the compacted data, we can reduce the size of incoming requests and save you money on inference.

- [**Unified SDK**](/getting-started/openpipe-sdk): Switching requests from your previous LLM provider to your new model is as simple as changing the model name. All our models implement the OpenAI inference format, so you won't have to change how you parse its response.

Welcome to the OpenPipe community!
