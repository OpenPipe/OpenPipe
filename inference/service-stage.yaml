name: inference-stage
compute_config: openpipe5
cluster_env: llm-env:5

ray_serve_config:
  applications:
  - name: OpenPipe--mistral-ft-optimized-1227
    route_prefix: /
    import_path: aviary_private_endpoints.backend.server.run:router_application
    args:
      models: []
      multiplex_models:
        - deployment_config:
            autoscaling_config:
              min_replicas: 1
              initial_replicas: 1
              max_replicas: 8
              target_num_ongoing_requests_per_replica: 24
              metrics_interval_s: 10.0
              look_back_period_s: 30.0
              smoothing_factor: 0.5
              downscale_delay_s: 300.0
              upscale_delay_s: 15.0
            max_concurrent_queries: 64
            ray_actor_options:
              resources:
                accelerator_type_a100_40g: 0.01
          engine_config:
            model_id: OpenPipe/mistral-ft-optimized-1227
            hf_model_id: OpenPipe/mistral-ft-optimized-1227
            type: VLLMEngine
            engine_kwargs:
              trust_remote_code: true
              # max_num_batched_tokens: 8192
              max_num_seqs: 64
              gpu_memory_utilization: 0.95
              enable_cuda_graph: true
              num_tokenizer_actors: 2
              enable_lora: true
              max_lora_rank: 8
              max_loras: 24
            max_total_tokens: 8192
            generation:
              prompt_format:
                system: "{instruction}"
                assistant: "{instruction}"
                user: "{instruction}"
                trailing_assistant: ""
              stopping_sequences: ["<unk>", "</s>"]
          scaling_config:
            num_workers: 1
            num_gpus_per_worker: 1
            num_cpus_per_worker: 8
            placement_strategy: "STRICT_PACK"
            resources_per_worker:
              accelerator_type_a100_40g: 0.01
          multiplex_config:
            # Number of LoRAs to serve concurrently
            max_num_models_per_replica: 24
      dynamic_lora_loading_path: s3://user-models-pl-prod-5e7392e/models/
