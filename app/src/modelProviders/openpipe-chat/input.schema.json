{
  "type": "object",
  "properties": {
    "model": {
      "description": "ID of the model to use.",
      "example": "Open-Orca/OpenOrcaxOpenChat-Preview2-13B",
      "type": "string",
      "enum": ["Open-Orca/OpenOrcaxOpenChat-Preview2-13B"]
    },
    "messages": {
      "description": "A list of messages comprising the conversation so far.",
      "type": "array",
      "minItems": 1,
      "items": {
        "type": "object",
        "properties": {
          "role": {
            "type": "string",
            "enum": ["system", "user", "assistant"],
            "description": "The role of the messages author. One of `system`, `user`, or `assistant`."
          },
          "content": {
            "type": "string",
            "description": "The contents of the message. `content` is required for all messages."
          }
        },
        "required": ["role", "content"]
      }
    },
    "temperature": {
      "type": "number",
      "minimum": 0,
      "maximum": 2,
      "default": 1,
      "example": 1,
      "nullable": true,
      "description": "What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.\n\nWe generally recommend altering this or `top_p` but not both.\n"
    },
    "top_p": {
      "type": "number",
      "minimum": 0,
      "maximum": 1,
      "default": 1,
      "example": 1,
      "nullable": true,
      "description": "An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.\n\nWe generally recommend altering this or `temperature` but not both.\n"
    },
    "stop": {
      "description": "Up to 4 sequences where the API will stop generating further tokens.\n",
      "default": null,
      "oneOf": [
        {
          "type": "string",
          "nullable": true
        },
        {
          "type": "array",
          "minItems": 1,
          "maxItems": 4,
          "items": {
            "type": "string"
          }
        }
      ]
    },
    "max_tokens": {
      "description": "The maximum number of [tokens](/tokenizer) to generate in the chat completion.\n\nThe total length of input tokens and generated tokens is limited by the model's context length. [Example Python code](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb) for counting tokens.\n",
      "type": "integer"
    },
    "presence_penalty": {
      "type": "number",
      "default": 0,
      "minimum": -2,
      "maximum": 2,
      "nullable": true,
      "description": "Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.\n\n[See more information about frequency and presence penalties.](/docs/api-reference/parameter-details)\n"
    },
    "frequency_penalty": {
      "type": "number",
      "default": 0,
      "minimum": -2,
      "maximum": 2,
      "nullable": true,
      "description": "Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.\n\n[See more information about frequency and presence penalties.](/docs/api-reference/parameter-details)\n"
    }
  },
  "required": ["model", "messages"]
}
